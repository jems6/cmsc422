{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Reading 20 Newsgroups dataset from /home/jems/cmsc422/p1/20_newsgroups...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m C, tf, df, tf_idf, dataset \u001b[38;5;66;03m# treat tf_idf as w\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m C, tf, df, tf_idf, D \u001b[38;5;241m=\u001b[39m \u001b[43mread_20_newsgroups\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/jems/cmsc422/p1/20_newsgroups\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m train, test \u001b[38;5;241m=\u001b[39m {}, {}\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Split dataset into train and test\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m, in \u001b[0;36mread_20_newsgroups\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;66;03m# Update document frequency (DF)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_counter:\n\u001b[0;32m---> 38\u001b[0m             document_count[word] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m word_counter[word]\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError reading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def read_20_newsgroups(directory):\n",
    "    print(f' Reading 20 Newsgroups dataset from {directory}...')\n",
    "    word_counter = Counter()\n",
    "    dataset = {}\n",
    "    document_count = defaultdict(int)  # Track how many documents contain each word (DF)\n",
    "    total_documents = 0  # Total number of documents\n",
    "\n",
    "    # To store TF and DF\n",
    "    tf = defaultdict(list)  # Term Frequency for each class and document\n",
    "\n",
    "    for curr_dir, classes, files in os.walk(directory):\n",
    "        curr_class = curr_dir.rsplit('/', 1)[-1]\n",
    "        dataset[curr_class] = []\n",
    "        for file in files:\n",
    "            total_documents += 1\n",
    "            file_path = os.path.join(curr_dir, file)\n",
    "            read_file = []\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    for line in f:\n",
    "                        bad = ['Newsgroup', 'document_id', 'From', 'Subject']\n",
    "                        if any(line.startswith(badd) for badd in bad):\n",
    "                            continue\n",
    "                        \n",
    "                        # Process each line\n",
    "                        words = re.findall(r'\\b\\w+\\b', line.lower())  # Convert to lowercase to count case-insensitively\n",
    "                        word_counter.update(words)  # Update the counter with words from the line\n",
    "                        read_file.extend(words)\n",
    "                        \n",
    "                    unique_words = set(read_file)  # Get unique words for this document\n",
    "                    for word in unique_words:\n",
    "                        document_count[word] += 1  # Increment DF for the word\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "            dataset[curr_class].append(read_file)\n",
    "    print(f' Finished reading {total_documents} documents from {len(dataset)} classes.')\n",
    "    # Remove top 300 most common words (stop words)\n",
    "    stop_words = set([word[0] for word in word_counter.most_common(300)])\n",
    "    del dataset['20_newsgroups']\n",
    "    for c, files in dataset.items():\n",
    "        for file in files:\n",
    "            file[:] = [word for word in file if word not in stop_words]\n",
    "\n",
    "    # Use the next 500 most common words\n",
    "    next_500_words = set(word[0] for word in word_counter.most_common(800)[300:800])\n",
    "    for c, files in dataset.items():\n",
    "        for file in files:\n",
    "            file[:] = [word for word in file if word in next_500_words]\n",
    "    print(f'finished sorting out the words')\n",
    "    # Compute and store TF and DF\n",
    "    for c, files in dataset.items():\n",
    "        for file in files:\n",
    "            word_freq_in_file = Counter(file)\n",
    "            total_words_in_file = len(file)\n",
    "            \n",
    "            file_tf = {}  # To store TF for this document\n",
    "            \n",
    "            for word in file:\n",
    "                # Calculate TF for the current word\n",
    "                tf_value = word_freq_in_file[word] / total_words_in_file\n",
    "                file_tf[word] = tf_value\n",
    "            \n",
    "            # Append the TF for this document to the list for the class\n",
    "            tf[c].append(file_tf)\n",
    "\n",
    "    # DF is already tracked in `document_count`, which is the document frequency\n",
    "    df = document_count\n",
    "\n",
    "    # Now compute TF-IDF for each word in each document\n",
    "    tf_idf = defaultdict(list)\n",
    "    N = total_documents  # Total number of documents\n",
    "\n",
    "    for c, files in dataset.items():\n",
    "        for file_tf in tf[c]:\n",
    "            file_tfidf = {}\n",
    "            for word, tf_value in file_tf.items():\n",
    "                df_value = df[word]  # Get document frequency for the word\n",
    "                idf_value = math.log10(N / df_value)  # Calculate IDF\n",
    "                tfidf_value = math.log10(1 + tf_value) * idf_value  # Apply the TF-IDF formula\n",
    "                file_tfidf[word] = tfidf_value\n",
    "            tf_idf[c].append(file_tfidf)\n",
    "    print(f' Finished computing TF-IDF for {N} documents.')\n",
    "    # print(f'TF-IDF : \\n {tf_idf}')\n",
    "    # print(f'TF : \\n {tf}')\n",
    "    # print(f'DF : \\n {df}')\n",
    "    C = list(dataset.keys())  # List of classes\n",
    "    return C, tf, df, tf_idf, dataset # treat tf_idf as w\n",
    "\n",
    "# Call the function\n",
    "C, tf, df, tf_idf, D = read_20_newsgroups(\"/home/jems/cmsc422/p1/20_newsgroups\")\n",
    "\n",
    "train, test = {}, {}\n",
    "# Split dataset into train and test\n",
    "for c, vectors in tf_idf.items():\n",
    "    train[c] = vectors[:len(vectors)//2]\n",
    "    test[c] = vectors[len(vectors)//2:]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in C:\n",
    "    print(f\"{train[c]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "Y = np.full((len(C), len(C)), -1)\n",
    "\n",
    "for i in range(len(C)):\n",
    "    Y[i][i] = 1\n",
    "\n",
    "\n",
    "# for every class, for every file belonging to that class, attach the tf-idf vector\n",
    "# tf_idf = w\n",
    "# futher processing may need to be done for this as we may need to restructure tf_idf into train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qpsolvers import solve_qp\n",
    "\n",
    "def train_svm(X, Y, C):\n",
    "    samples, features = X.shape\n",
    "    Y = Y.astype(float)\n",
    "    K = np.dot(X.T, X)\n",
    "    P = np.outer(Y, Y) * K \n",
    "    G = np.vstack((np.eye(samples) * -1, np.eye(samples)))\n",
    "    H = np.vstack((np.zeros(samples), np.full(samples, C)))\n",
    "    Q = np.full(samples, -1)\n",
    "    A = Y.reshape(1, -1)\n",
    "    b = np.array([0.0])\n",
    "    \n",
    "    alphas = solve_qp(P, Q, G, H, A, b, solver='osqp')\n",
    "    w = np.sum(alphas[:, None] * Y[:, None] * X, axis=0)\n",
    "\n",
    "    # Find support vectors (where 0 < alpha_i < C)\n",
    "    support_vector_indices = np.where((alphas > 1e-5) & (alphas < C))[0]\n",
    "    support_vectors = X[support_vector_indices]\n",
    "    support_alphas = alphas[support_vector_indices]\n",
    "    support_labels = Y[support_vector_indices]\n",
    "\n",
    "    # Compute the bias term b using a support vector\n",
    "    b = np.mean([y_i - np.dot(w, x_i) for (y_i, x_i) in zip(support_labels, support_vectors)])\n",
    "\n",
    "    return w, b, support_vectors, support_alphas, support_labels\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b, s_vectors, s_alphas, s_labels = train_svm(train, Y, 1) # Do this for every class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svm_per_class = {}\n",
    "\n",
    "for i in range(len(C)):\n",
    "    w, b, s_vectors, s_alphas, s_labels = train_svm(train[C[i]], Y[i], 1)\n",
    "    train_svm_per_class[C[i]] = (w, b, s_vectors, s_alphas, s_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "sv = [alpha if alpha > 1e-5 else 0 for alpha in alphas]\n",
    "ind = np.arange(len(alphas))[sv]\n",
    "sv_alphas = alphas[sv]\n",
    "sv_x = X[sv]\n",
    "sv_y = Y[sv]\n",
    "\n",
    "w = np.sum(alpha_sv * sv_y).reshape(-1,1 ) * sv_x, axis=0)\n",
    "b = np.mean(sv_y - sv_x @ w)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
