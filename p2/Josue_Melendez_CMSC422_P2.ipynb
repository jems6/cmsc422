{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Reading 20 Newsgroups dataset from /home/jems/cmsc422/p1/20_newsgroups...\n",
      " Finished reading 19997 documents from 21 classes.\n",
      "finished sorting out the words\n",
      " Finished computing TF-IDF for 19997 documents.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "def read_20_newsgroups(directory):\n",
    "    print(f' Reading 20 Newsgroups dataset from {directory}...')\n",
    "    word_counter = Counter()\n",
    "    dataset = {}\n",
    "    document_count = defaultdict(int)  \n",
    "    total_documents = 0 \n",
    "\n",
    "    tf = defaultdict(list) \n",
    "\n",
    "    for curr_dir, classes, files in os.walk(directory):\n",
    "        curr_class = curr_dir.rsplit('/', 1)[-1]\n",
    "        dataset[curr_class] = []\n",
    "        # process each file, take the word and document frequency and store them in the dataset\n",
    "        for file in files:\n",
    "            total_documents += 1\n",
    "            file_path = os.path.join(curr_dir, file)\n",
    "            read_file = []\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    for line in f:\n",
    "                        bad = ['Newsgroup', 'document_id', 'From', 'Subject']\n",
    "                        if any(line.startswith(badd) for badd in bad):\n",
    "                            continue\n",
    "                        \n",
    "                        \n",
    "                        words = re.findall(r'\\b\\w+\\b', line.lower())  \n",
    "                        word_counter.update(words)  # Update the counter with words from the line\n",
    "                        read_file.extend(words)\n",
    "                        \n",
    "                    unique_words = set(read_file)  # Get unique words for this document\n",
    "                    for word in unique_words:\n",
    "                        document_count[word] += 1  # Increment DF for the word\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "            dataset[curr_class].append(read_file)\n",
    "    print(f' Finished reading {total_documents} documents from {len(dataset)} classes.')\n",
    "    # Remove top 300 most common words (stop words)\n",
    "    stop_words = set([word[0] for word in word_counter.most_common(300)])\n",
    "    del dataset['20_newsgroups']\n",
    "    for c, files in dataset.items():\n",
    "        for file in files:\n",
    "            file[:] = [word for word in file if word not in stop_words]\n",
    "\n",
    "    # Use the next 500 most common words\n",
    "    next_500_words = set(word[0] for word in word_counter.most_common(800)[300:800])\n",
    "    for c, files in dataset.items():\n",
    "        for file in files:\n",
    "            file[:] = [word for word in file if word in next_500_words]\n",
    "    print(f'finished sorting out the words')\n",
    "    # Compute and store TF and DF\n",
    "    for c, files in dataset.items():\n",
    "        for file in files:\n",
    "            word_freq_in_file = Counter(file)\n",
    "            total_words_in_file = len(file)\n",
    "            \n",
    "            file_tf = {}  # To store TF for this document\n",
    "            \n",
    "            for word in file:\n",
    "                # Calculate TF for the current word\n",
    "                tf_value = word_freq_in_file[word] / total_words_in_file\n",
    "                file_tf[word] = tf_value\n",
    "            \n",
    "            # Append the TF for this document to the list for the class\n",
    "            tf[c].append(file_tf)\n",
    "\n",
    "    # DF is already tracked in `document_count`, which is the document frequency\n",
    "    df = document_count\n",
    "\n",
    "    # Now compute TF-IDF for each word in each document\n",
    "    tf_idf = defaultdict(list)\n",
    "    N = total_documents  # Total number of documents\n",
    "\n",
    "    for c, files in dataset.items():\n",
    "        for file_tf in tf[c]:\n",
    "            \n",
    "            # Initialize a vector of zeros with a length equal to next_500_words\n",
    "            tfidf_vector = np.zeros(len(next_500_words))\n",
    "            \n",
    "            # Create an index mapping for the next_500_words\n",
    "            word_to_index = {word: idx for idx, word in enumerate(next_500_words)}\n",
    "            \n",
    "            # For each word in the document, compute TF-IDF and place it in the correct position\n",
    "            for word, tf_value in file_tf.items():\n",
    "                if word in next_500_words:\n",
    "                    df_value = df[word]  # Get document frequency for the word\n",
    "                    idf_value = math.log10(N / df_value)  # Calculate IDF\n",
    "                    tfidf_value = math.log10(1 + tf_value) * idf_value  # Apply the TF-IDF formula\n",
    "                    \n",
    "                    # Place the TF-IDF value in the corresponding index\n",
    "                    tfidf_vector[word_to_index[word]] = tfidf_value\n",
    "            \n",
    "            # Append the fixed-length TF-IDF vector to the list for the class\n",
    "            tf_idf[c].append(tfidf_vector)\n",
    "\n",
    "    print(f' Finished computing TF-IDF for {N} documents.')\n",
    "    # print(f'TF-IDF : \\n {tf_idf}')\n",
    "    # print(f'TF : \\n {tf}')\n",
    "    # print(f'DF : \\n {df}')\n",
    "    C = list(dataset.keys())  # List of classes\n",
    "    return C, tf, df, tf_idf, dataset # treat tf_idf as w\n",
    "\n",
    "# Call the function\n",
    "Classes, tf, df, tf_idf, D = read_20_newsgroups(\"/home/jems/cmsc422/p1/20_newsgroups\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sci.crypt : 1000\n",
      "sci.space : 1000\n",
      "comp.sys.mac.hardware : 1000\n",
      "soc.religion.christian : 997\n",
      "talk.religion.misc : 1000\n",
      "talk.politics.misc : 1000\n",
      "comp.os.ms-windows.misc : 1000\n",
      "comp.windows.x : 1000\n",
      "misc.forsale : 1000\n",
      "comp.sys.ibm.pc.hardware : 1000\n",
      "rec.sport.hockey : 1000\n",
      "rec.motorcycles : 1000\n",
      "comp.graphics : 1000\n",
      "rec.sport.baseball : 1000\n",
      "sci.electronics : 1000\n",
      "sci.med : 1000\n",
      "talk.politics.guns : 1000\n",
      "talk.politics.mideast : 1000\n",
      "alt.atheism : 1000\n",
      "rec.autos : 1000\n"
     ]
    }
   ],
   "source": [
    "for c in tf_idf:\n",
    "    print(f'{c} : {len(tf_idf[c])}')\n",
    "\n",
    "# soc.religion.christian has 3 less documents than the rest so we append 3 filler arrays to make it 500\n",
    "filler = np.zeros(500)\n",
    "for i in range(3):\n",
    "    tf_idf['soc.religion.christian'].append(filler)\n",
    "\n",
    "for c in tf_idf:\n",
    "    for file in tf_idf[c]:\n",
    "        if len(file) != 500:\n",
    "            print(f'{c} : {len(file)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(Classes)}')\n",
    "\n",
    "print(f'{len([np.array(tf_idf[c]) for c in Classes])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1000}\n",
      "(20, 1000)\n"
     ]
    }
   ],
   "source": [
    "class_dict = {c: i for i, c in enumerate(Classes)} # dictionary to convert class names to indices\n",
    "\n",
    "temp = np.array([np.array(tf_idf[c]) for c in Classes]) # convert the dictionary to a numpy array\n",
    "\n",
    "print({len(temp[i]) for i in range(len(temp))})\n",
    "# At this stage our X has the shape (20, 1000, 500) where 20 is the number of classes, 1000 is the number of documents and 500 is the number of words\n",
    "# We need to convert this to a shape of (20, 1000) to use in our caculations and we do this by taking the dot product of the word vectors for each document\n",
    "X = []\n",
    "for i in range(len(temp)):\n",
    "    dot_product = []\n",
    "    for j in range(len(temp[i])):\n",
    "        dot_product.append(np.dot(temp[i][j], temp[i][j]))\n",
    "    X.append(np.array(dot_product))\n",
    "X = np.array(X)\n",
    "print(X.shape)\n",
    "# After converting our X into a 2D matrix, we split the data into training and testing data\n",
    "train, test = [], []\n",
    "for i in range(len(X)):\n",
    "    train.append(X[i][len(X[i])//2:])\n",
    "    test.append(X[i][:len(X[i])//2])\n",
    "\n",
    "train = np.array(train)\n",
    "test = np.array(test)\n",
    "\n",
    "# We set up our classification into a one hot encoding format allowing us to easily access the class of a document\n",
    "Y = np.full((len(Classes), len(Classes)), -1)\n",
    "for i in range(len(Classes)):\n",
    "    Y[i][i] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qpsolvers import solve_qp\n",
    "import osqp\n",
    "from scipy import sparse\n",
    "\n",
    "# Here we define our SVM training function\n",
    "def train_svm(X, Y, C):\n",
    "    samples, features = X.shape\n",
    "    Y = Y.astype(float)\n",
    "    P = np.outer(Y, Y) * np.dot(X, X.T) \n",
    "    epsilon = 1e-5\n",
    "    P += np.eye(samples) * epsilon\n",
    "    P_sparse = sparse.csr_matrix(P)\n",
    "    G = np.vstack((np.eye(samples) * -1, np.eye(samples)))\n",
    "    G_sparse = sparse.csr_matrix(G)\n",
    "    H = np.vstack((np.zeros(samples), np.ones(samples) * C)).flatten()\n",
    "    Q = np.full(samples, -1)\n",
    "    A = Y.reshape(1, -1)\n",
    "    A_sparse = sparse.csr_matrix(A)\n",
    "    b = np.array([0.0])\n",
    "    \n",
    "    # we extract our supporting vectors and their corresponding alphas and labels\n",
    "    alphas = solve_qp(P_sparse, Q, G_sparse, H, A_sparse, b, solver='osqp')\n",
    "    vector_indexes = alphas > 1e-5\n",
    "    support_alphas = alphas[vector_indexes]\n",
    "    support_vectors = X[vector_indexes]\n",
    "    support_labels = Y[vector_indexes]\n",
    "    \n",
    "    w = np.sum((support_alphas * support_labels).reshape(-1, 1) * support_vectors, axis=0)\n",
    "    b = np.mean(support_labels - support_vectors @ w)\n",
    "\n",
    "    return w, b, support_vectors, support_alphas, support_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "[-1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "[-1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "[-1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "[-1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "[-1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "[-1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "[-1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1]\n"
     ]
    }
   ],
   "source": [
    "# We define our slack variable and c and then for every class we train an SVM\n",
    "c = float(100)\n",
    "train_svm_per_class = {}\n",
    "for i, clas in enumerate(Classes):\n",
    "    w, b, s_vectors, s_alphas, s_labels = train_svm(train, Y[i], c)\n",
    "    print(f'{Y[i]}')\n",
    "    train_svm_per_class[clas] = {'w': w, 'b': b, 's_vectors': s_vectors, 's_alphas': s_alphas, 's_labels': s_labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sci.crypt : [47.14783055  1.80215108  0.41143787 14.66593773  1.7354826   1.56876368\n",
      "  0.19266712  1.45775464  2.53560914  0.70473273  4.27977174  1.98165104\n",
      "  0.70621586  2.44480741  1.86878985  5.71154535  1.95500757  2.16404944\n",
      "  0.96144874]\n",
      "sci.space : [1.79611079e+00 2.79641236e+01 9.41615617e-03 2.67023062e+00\n",
      " 2.53933994e+00 1.82636895e+00 2.06221874e+00 1.06966383e+00\n",
      " 2.27596478e+00 6.07117751e-01 1.97133755e+00 4.15075295e-01\n",
      " 1.55683896e+00 2.68946843e+00 1.69987262e+00 2.92950677e+00\n",
      " 7.57513980e-01 5.82301115e-01 5.05768953e-01]\n",
      "comp.sys.mac.hardware : [ 0.25722231 27.27529844  0.29942356  1.665049    0.81454482  0.84192445\n",
      "  3.51932311  1.6980517   3.57773812  1.35153916  2.33481985  2.50084184\n",
      "  0.0313701   3.03406452  0.20892071  1.27923057  2.74569405  1.11556396]\n",
      "soc.religion.christian : [1.37449556e+01 1.75364058e+00 5.48962222e+01 4.97283210e+00\n",
      " 1.26106646e+01 1.27918358e-01 4.80895785e-01 3.36202509e+00\n",
      " 4.06665986e-02 9.41436027e+00 3.28800691e+00 4.06392727e+00\n",
      " 1.03636686e+00]\n",
      "talk.religion.misc : [1.54321193e+00 2.40282080e+00 1.44595669e+00 5.59784291e+00\n",
      " 2.06315547e+01 1.15768906e-05 1.87302401e+00 1.11090425e+00\n",
      " 1.35928640e+00 1.17600149e-04 1.77019874e+00 6.91291614e-01\n",
      " 8.16801441e-01 1.38922086e+00 6.31161115e-01]\n",
      "talk.politics.misc : [1.56745325e+00 1.67218383e+00 1.05461503e+00 1.32243894e+01\n",
      " 3.80913975e+01 9.14229716e-01 9.08710453e-04 9.91334159e-01\n",
      " 1.99397132e-01 2.04692426e+00 8.67849910e-01 2.52307665e+00\n",
      " 2.55689554e+00 2.94219049e+00 4.76909484e+00 1.25289218e+00\n",
      " 1.50925237e+00]\n",
      "comp.os.ms-windows.misc : [7.33885704e-02 1.96626644e+00 7.71100929e-01 1.56641905e-01\n",
      " 1.18994423e-05 5.95281787e+00 5.70426478e-01 8.50470949e-02\n",
      " 1.78987404e-05 1.77429254e-01 7.33067970e-01 1.46468025e-01\n",
      " 7.22176621e-01 1.24044893e-05 5.50718101e-01 1.10392423e-05]\n",
      "comp.windows.x : [ 0.99942599  3.41652288  1.86396103  0.5094833   0.62379491 22.91797636\n",
      "  0.91215676  3.24896378  0.94769845  2.07891278  1.75635263  1.42905027\n",
      "  1.69295844  0.20161901  1.43630782  1.80077886]\n",
      "misc.forsale : [5.67703868e-01 1.62645224e+00 1.08498448e-04 5.52197137e-01\n",
      " 9.79827132e-02 8.68914673e-01 1.28354070e+01 1.21958808e+00\n",
      " 7.78523435e-01 1.09480234e+00 2.65811240e+00 8.86507826e-01\n",
      " 4.78208034e-01 6.23916145e-01 1.03111825e+00 3.51410834e-01]\n",
      "comp.sys.ibm.pc.hardware : [2.09427428e+00 2.04295995e+00 3.68426065e+00 4.35521531e-05\n",
      " 1.35661356e+00 7.17095823e-01 3.30026882e+00 1.37241703e+00\n",
      " 3.08938488e+01 2.16272093e+00 8.56742154e-01 1.11215013e+00\n",
      " 2.44279121e+00 5.71784411e-01 2.80557446e+00 9.82210995e-01\n",
      " 2.31066095e+00 1.42299479e+00 1.65840686e+00]\n",
      "rec.sport.hockey : [ 0.61187613  0.53880771  1.40795639  1.42228666  0.04174832  1.01240754\n",
      "  0.9426218   2.17453802 18.50068844  2.95261034  1.90672915  0.25859607\n",
      "  0.82242252  1.60612933  1.19686119  0.28025863  1.32489178]\n",
      "rec.motorcycles : [ 4.26722094  1.96863599  2.4166575   5.29166714  0.14186906  0.30427253\n",
      "  2.25402563  1.85590131  0.99448021  2.98145045 40.64592296  3.11753034\n",
      "  2.80785213  0.67062345  3.78573849  2.38952389  1.4120687   3.98687857]\n",
      "comp.graphics : [9.84975317e-01 1.47773147e-01 2.52358903e+00 1.41274697e-05\n",
      " 1.37920960e+00 2.18110426e-01 1.77243497e+00 2.79720313e+00\n",
      " 1.07130946e+00 1.83640656e+00 2.79324338e+00 2.55397195e+01\n",
      " 1.15812530e-01 1.82709848e+00 1.42965288e+00 1.35756418e+00\n",
      " 1.41906848e+00 1.36025911e+00 2.50605696e+00]\n",
      "rec.sport.baseball : [3.03869878e-01 1.43937271e+00 8.57843207e-02 1.71708905e+00\n",
      " 6.70666726e-01 8.09696760e-01 1.50795787e+00 9.42509921e-01\n",
      " 2.48523376e+00 2.61857298e-01 2.66211907e+00 1.51875419e-01\n",
      " 1.85313110e+01 1.10227488e+00 1.37783130e+00 7.41948121e-05\n",
      " 1.17675509e+00 4.19063283e-01 1.41731946e+00]\n",
      "sci.electronics : [ 2.40508497  2.68946408  3.07106536  0.30350376  0.76472544  2.56033256\n",
      "  0.16958257  0.72862394  0.51140068  0.81479877  0.61747495  1.83516446\n",
      "  1.0721933  21.05321942  0.61326091  0.1208102   1.53819026  0.31029509\n",
      "  0.92727732]\n",
      "sci.med : [1.67305139e+00 1.62437932e+00 1.86836571e-01 9.13715213e-01\n",
      " 9.29014586e-01 2.37209234e+00 7.83856833e-01 1.74716777e+00\n",
      " 8.78458754e-01 2.77527893e+00 1.56885838e+00 3.68973119e+00\n",
      " 1.43752861e+00 1.33762312e+00 5.83866956e-01 2.35447261e+01\n",
      " 4.80484053e-05 8.35762614e-01 2.07450509e-01]\n",
      "talk.politics.guns : [5.61753337e+00 2.75088644e+00 1.31970965e+00 1.01964125e+01\n",
      " 1.46098791e+00 2.88544263e+00 3.26212808e-01 6.77092222e-01\n",
      " 1.02149253e+00 2.08772967e+00 1.96905656e+00 1.12781963e-04\n",
      " 3.51311654e-02 3.44449637e+01 1.16403511e+00 1.87168917e+00\n",
      " 1.06183214e+00]\n",
      "talk.politics.mideast : [1.73551670e+00 6.48353956e-01 4.47629638e+00 7.50418877e-01\n",
      " 4.58293053e+00 5.12860360e-01 3.09327454e-05 1.68494326e+00\n",
      " 1.78981912e+00 9.43908417e-01 8.91912942e-01 1.31359548e+00\n",
      " 1.11214504e+00 1.20216297e+00 1.10803038e+00 3.01917164e+01\n",
      " 5.74332572e+00 1.69548559e+00]\n",
      "alt.atheism : [ 2.02544463  0.39039193  3.1025399   4.60397043  1.24616793  1.56132518\n",
      "  0.17986652  1.45668653  0.26729912  1.59878319  0.3990874   0.28970315\n",
      "  1.05627064  1.81891716  6.10975654 26.10586327]\n",
      "rec.autos : [9.43795013e-01 5.08573327e-01 1.24050026e+00 2.33561903e+00\n",
      " 1.54519163e+00 1.10125601e-02 1.92033503e+00 7.32370869e-01\n",
      " 1.71102914e+00 1.34017714e+00 3.98650139e+00 2.64217125e+00\n",
      " 1.48052839e+00 9.71638725e-01 3.06941760e-01 1.15624731e+00\n",
      " 1.87804679e+00 2.47106110e+01]\n"
     ]
    }
   ],
   "source": [
    "for clas, svm in train_svm_per_class.items():\n",
    "    print(f'{clas} : {svm[\"s_alphas\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every class we predict the SVM\n",
    "def predict_svm(X, w, b):\n",
    "    return X @ w + b\n",
    "\n",
    "predict_svm_per_class = {}\n",
    "for clas, svm in train_svm_per_class.items():\n",
    "    predict_svm_per_class[clas] = lambda X, svm=svm: predict_svm(test, svm['w'], svm['b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sci.crypt : 4\n",
      "Correct score = 0\n",
      "sci.space : 8\n",
      "Correct score = 1\n",
      "comp.sys.mac.hardware : 12\n",
      "Correct score = 2\n",
      "soc.religion.christian : 3\n",
      "Correct score = 3\n",
      "talk.religion.misc : 18\n",
      "Correct score = 4\n",
      "talk.politics.misc : 15\n",
      "Correct score = 5\n",
      "comp.os.ms-windows.misc : 0\n",
      "Correct score = 6\n",
      "comp.windows.x : 10\n",
      "Correct score = 7\n",
      "misc.forsale : 6\n",
      "Correct score = 8\n",
      "comp.sys.ibm.pc.hardware : 6\n",
      "Correct score = 9\n",
      "rec.sport.hockey : 0\n",
      "Correct score = 10\n",
      "rec.motorcycles : 15\n",
      "Correct score = 11\n",
      "comp.graphics : 8\n",
      "Correct score = 12\n",
      "rec.sport.baseball : 8\n",
      "Correct score = 13\n",
      "sci.electronics : 4\n",
      "Correct score = 14\n",
      "sci.med : 1\n",
      "Correct score = 15\n",
      "talk.politics.guns : 6\n",
      "Correct score = 16\n",
      "talk.politics.mideast : 10\n",
      "Correct score = 17\n",
      "alt.atheism : 6\n",
      "Correct score = 18\n",
      "rec.autos : 4\n",
      "Correct score = 19\n",
      "Accuracy: 0.05\n"
     ]
    }
   ],
   "source": [
    "# We check the acccuracy of our SVM\n",
    "correct = 0\n",
    "total = 0\n",
    "for clas, svm in predict_svm_per_class.items():\n",
    "    total += 1\n",
    "    result = np.argmax(svm(test))\n",
    "    if class_dict[clas] == result:\n",
    "        correct += 1\n",
    "    print(f'{clas} : {result}')\n",
    "    print(f'Correct score = {class_dict[clas]}')\n",
    "print(f'Accuracy: {correct/total}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we repeat these steps for the kernelized SVM\n",
    "def svm_with_kernel(X, Y, C, c=0, d=2):\n",
    "    poly_kernel = lambda x, y, c, d: (np.dot(x, y.T) + c) ** d\n",
    "    samples, features = X.shape\n",
    "    Y = Y.astype(float)\n",
    "    P = np.outer(Y, Y) * poly_kernel(X, X, c, d)\n",
    "    epsilon = 1e-5\n",
    "    P += np.eye(samples) * epsilon\n",
    "    P_sparse = sparse.csr_matrix(P)\n",
    "    G = np.vstack((np.eye(samples) * -1, np.eye(samples)))\n",
    "    G_sparse = sparse.csr_matrix(G)\n",
    "    H = np.vstack((np.zeros(samples), np.ones(samples) * C)).flatten()\n",
    "    Q = np.full(samples, -1)\n",
    "    A = Y.reshape(1, -1)\n",
    "    A_sparse = sparse.csr_matrix(A)\n",
    "    b = np.array([0.0])\n",
    "    \n",
    "    alphas = solve_qp(P_sparse, Q, G_sparse, H, A_sparse, b, solver='osqp')\n",
    "    vector_indexes = alphas > 1e-5\n",
    "    support_alphas = alphas[vector_indexes]\n",
    "    support_vectors = X[vector_indexes]\n",
    "    support_labels = Y[vector_indexes]\n",
    "    \n",
    "    w = np.sum((support_alphas * support_labels).reshape(-1, 1) * support_vectors, axis=0)\n",
    "    b = np.mean(support_labels - support_vectors @ w)\n",
    "\n",
    "    return w, b, support_vectors, support_alphas, support_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sci.crypt : 13\n",
      "Correct score = 0\n",
      "sci.space : 8\n",
      "Correct score = 1\n",
      "comp.sys.mac.hardware : 12\n",
      "Correct score = 2\n",
      "soc.religion.christian : 3\n",
      "Correct score = 3\n",
      "talk.religion.misc : 6\n",
      "Correct score = 4\n",
      "talk.politics.misc : 15\n",
      "Correct score = 5\n",
      "comp.os.ms-windows.misc : 0\n",
      "Correct score = 6\n",
      "comp.windows.x : 10\n",
      "Correct score = 7\n",
      "misc.forsale : 6\n",
      "Correct score = 8\n",
      "comp.sys.ibm.pc.hardware : 6\n",
      "Correct score = 9\n",
      "rec.sport.hockey : 0\n",
      "Correct score = 10\n",
      "rec.motorcycles : 15\n",
      "Correct score = 11\n",
      "comp.graphics : 8\n",
      "Correct score = 12\n",
      "rec.sport.baseball : 14\n",
      "Correct score = 13\n",
      "sci.electronics : 4\n",
      "Correct score = 14\n",
      "sci.med : 1\n",
      "Correct score = 15\n",
      "talk.politics.guns : 6\n",
      "Correct score = 16\n",
      "talk.politics.mideast : 10\n",
      "Correct score = 17\n",
      "alt.atheism : 6\n",
      "Correct score = 18\n",
      "rec.autos : 4\n",
      "Correct score = 19\n",
      "Accuracy: 0.05\n"
     ]
    }
   ],
   "source": [
    "c = float(100)\n",
    "kernel_svm_per_class = {}\n",
    "for i, clas in enumerate(Classes):\n",
    "    w, b, s_vectors, s_alphas, s_labels = svm_with_kernel(train, Y[i], c)\n",
    "    kernel_svm_per_class[clas] = {'w': w, 'b': b, 's_vectors': s_vectors, 's_alphas': s_alphas, 's_labels': s_labels}\n",
    "\n",
    "predict_kernel_svm_per_class = {}\n",
    "for clas, svm in kernel_svm_per_class.items():\n",
    "    predict_kernel_svm_per_class[clas] = lambda X, svm=svm: predict_svm(test, svm['w'], svm['b'])\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for clas, svm in predict_kernel_svm_per_class.items():\n",
    "    total += 1\n",
    "    result = np.argmax(svm(test))\n",
    "    if class_dict[clas] == result:\n",
    "        correct += 1\n",
    "    print(f'{clas} : {result}')\n",
    "    print(f'Correct score = {class_dict[clas]}')\n",
    "print(f'Accuracy: {correct/total}') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
