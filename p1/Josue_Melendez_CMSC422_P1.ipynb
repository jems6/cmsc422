{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['20_newsgroups', 'sci.crypt', 'sci.space', 'comp.sys.mac.hardware', 'soc.religion.christian', 'talk.religion.misc', 'talk.politics.misc', 'comp.os.ms-windows.misc', 'comp.windows.x', 'misc.forsale', 'comp.sys.ibm.pc.hardware', 'rec.sport.hockey', 'rec.motorcycles', 'comp.graphics', 'rec.sport.baseball', 'sci.electronics', 'sci.med', 'talk.politics.guns', 'talk.politics.mideast', 'alt.atheism', 'rec.autos'])\n",
      "classes=[]\n",
      "Class sci.crypt has 1000 files\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m             file\u001b[38;5;241m.\u001b[39mremove(stop_words)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m C, dataset\n\u001b[0;32m---> 58\u001b[0m C, D \u001b[38;5;241m=\u001b[39m \u001b[43mread_20_newsgroups\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/jems/cmsc422/p1/20_newsgroups\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m train, test \u001b[38;5;241m=\u001b[39m {}, {}\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# split D in half to get train and test\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 55\u001b[0m, in \u001b[0;36mread_20_newsgroups\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[0;32m---> 55\u001b[0m         \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstop_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m C, dataset\n",
      "\u001b[0;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Pseudo Code\n",
    "\n",
    "# store the documents as lists of words, one after another, one list per doc\n",
    "\n",
    "# For each paper source in 20_newsgroups\n",
    "#     For each paper in the source\n",
    "#         remove the first 4 lines (lines starting with Newsgroup, document_id, From, Subject)\n",
    "#         save into dataset (source, paper)\n",
    "# for each source in dataset\n",
    "#     split in half, (train, test)\n",
    "\n",
    "# dataset complete, good to go\n",
    "\n",
    "def read_20_newsgroups(directory):\n",
    "    word_counter = Counter()\n",
    "\n",
    "    C = []\n",
    "    dataset = {}\n",
    "    for curr_dir, classes, files in os.walk(directory):\n",
    "        C = classes\n",
    "        #print(f\"Reading directory: {curr_dir}, files: {files}, dirs: {classes}\")\n",
    "        curr_class = curr_dir.rsplit('/', 1)[-1]\n",
    "        dataset[curr_class] = []\n",
    "        for file in files:\n",
    "            file_path = os.path.join(curr_dir, file)\n",
    "            # print(f\"Reading file: {file_path}\")\n",
    "            read_file = []\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    count = 0 # remove first four lines\n",
    "                    for line in f:\n",
    "                        if count < 4:\n",
    "                            count += 1\n",
    "                            continue\n",
    "                        # print(f'line: {line.strip()}')  # Process each line here\n",
    "                        words = re.findall(r'\\b\\w+\\b', line.lower())  # Convert to lowercase to count case-insensitively\n",
    "                        word_counter.update(words)  # Update the counter with words from the line\n",
    "                        read_file.extend(words)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "            dataset[curr_class].append(read_file)\n",
    "    stop_words = word_counter.most_common(200)\n",
    "    print(f'{dataset.keys()}')\n",
    "    del dataset['20_newsgroups']\n",
    "    print(f'{classes=}')\n",
    "    for c, files in dataset.items():\n",
    "        print(f\"Class {c} has {len(files)} files\")\n",
    "        for file in files:\n",
    "            for word in stop_words:\n",
    "                \n",
    "                if word[0] in file:\n",
    "                    file.remove(word[0])\n",
    "\n",
    "    return C, dataset\n",
    "C, D = read_20_newsgroups(\"/home/jems/cmsc422/p1/20_newsgroups\")\n",
    "train, test = {}, {}\n",
    "# split D in half to get train and test\n",
    "for c, files in D.items():\n",
    "    train[c] = files[:len(files)//2]\n",
    "    test[c] = files[len(files)//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo Code\n",
    "\n",
    "# Naive Bayes Implementations\n",
    "\n",
    "# returns V, log P(c), log P(w|c)\n",
    "def train_naive_bayes(D, C): # D is the dataset, C is the classes\n",
    "    # Initialize the count of each class\n",
    "    ndoc = sum([len(D[c]) for c in D]) # for class in the dataset count the number of documents and sum them\n",
    "    logprior = {} # initialize the logprior\n",
    "    loglikelihood = {} # initialize the loglikelihood\n",
    "    bigdoc = {} # initialize the bigdoc\n",
    "    V = set() # initialize the vocabulary\n",
    "    for c in C: # for each class in the classes\n",
    "        ndoc_c = len(D[c]) # count the number of documents in the class\n",
    "        logprior[c] = math.log(ndoc_c / ndoc)\n",
    "        D_c = set([word for doc in D[c] for word in doc]) # get the words in the class\n",
    "        bigdoc[c] = [doc for doc in D[c]]\n",
    "        V.update(D_c) # add the words to the vocabulary\n",
    "        for word in V:\n",
    "            pass\n",
    "\n",
    "\n",
    "    return V, logprior, loglikelihood\n",
    "\n",
    "V, lpc, lpwc = train_naive_bayes(C, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(testdoc, C, V, logprior, loglikelihood):\n",
    "    summ = {}\n",
    "    for c in C:\n",
    "        sum[c] = logprior[c]\n",
    "        for i in testdoc:\n",
    "            word = testdoc[i] # get the word, def wrong btw\n",
    "            if word in V:\n",
    "                sum[c] += loglikelihood[word][c] \n",
    "    return summ.index(max(summ)) # return the argmax\n",
    "\n",
    "holder = test_naive_bayes(test, C, V, lpc, lpwc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
